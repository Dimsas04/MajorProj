# from typing import Optional
# from crewai.tools import tool
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import time
# import os
# import sys
# import pandas as pd
# from dotenv import load_dotenv

# @tool()
# class AmazonScraperTool():
#     name: str = "amazon_scraper_tool"
#     description: str = "Scrapes customer reviews from Amazon product pages"

#     def __init__(self):
#         super().__init__(
#             name=self.name,
#             description=self.description,
#             func=self._scrape_reviews
#         )
        
#         # Load environment variables
#         load_dotenv()
#         self.username = os.getenv('NUMBER')
#         self.password = os.getenv('PASSWORD')
        
#         # Check if credentials are available
#         if not self.username or not self.password:
#             print("‚ö†Ô∏è Error: Missing credentials in .env file")
#             print("Please ensure your .env file contains:")
#             print("NUMBER=your_email_or_phone_number")
#             print("PASSWORD=your_amazon_password")

#     def __call__(self, url: str) -> str:
#         # wrap your existing `_scrape_reviews()` here
#         return self._scrape_reviews(url)

#     def _scrape_reviews(self, url: str, target_reviews: int = 50, product_name: Optional[str] = None) -> str:
#         """
#         Scrape reviews from Amazon product page
        
#         Args:
#             url: Amazon product URL
#             target_reviews: Number of reviews to scrape (default: 50)
#             product_name: Optional name to use for the product (default: extracted from page)
            
#         Returns:
#             Path to the CSV file containing the scraped reviews
#         """
#         # Check for credentials
#         if not self.username or not self.password:
#             return "Error: Missing Amazon credentials in .env file"
        
#         print(f"Starting to scrape {target_reviews} reviews from: {url}")
        
#         # Set up the driver and scrape reviews
#         try:
#             reviews_titles, reviews_texts, ratings = self._perform_scraping(url, target_reviews)
            
#             # Save to CSV
#             if not product_name:
#                 product_name = "Amazon Product"  # Default name if not provided
                
#             csv_path = self._save_to_csv(reviews_titles, reviews_texts, ratings, product_name)
            
#             return f"Successfully scraped {len(reviews_texts)} reviews. Data saved to {csv_path}"
        
#         except Exception as e:
#             return f"Error during scraping: {str(e)}"

#     def _perform_scraping(self, url, target_reviews):
#         """Internal method to handle the actual scraping logic"""
#         driver = self._setup_driver()
#         review_titles = []
#         reviews = []
#         ratings = []

#         try:
#             print(f"Opening URL: {url}")
#             driver.get(url)

#             # Wait and click on the "See all reviews" link
#             print("Looking for 'See all reviews' link...")
#             WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.XPATH, '//*[@id="reviews-medley-footer"]/div[2]/a'))
#             ).click()
            
#             # Wait for email input field and enter email
#             print("Logging in with provided credentials...")
#             email_input = WebDriverWait(driver, 20).until(
#                 EC.presence_of_element_located((By.ID, "ap_email_login"))
#             )
#             print(f"Using username: {self.username[:2]}...{self.username[-2:]}")
#             email_input.send_keys(self.username)
            
#             enter_button = WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.ID, "continue"))
#             )
#             enter_button.click()
            
#             password_input = WebDriverWait(driver, 20).until(
#                 EC.presence_of_element_located((By.ID, "ap_password"))
#             )
#             password_input.send_keys(self.password)
            
#             sign_in_button = WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.ID, "signInSubmit"))
#             )
#             sign_in_button.click()
            
#             page_count = 1
#             max_pages = 10  # Maximum number of pages to scrape
            
#             # Continue until we reach target_reviews or max_pages
#             while len(reviews) < target_reviews and page_count <= max_pages:
#                 print(f"Scraping page {page_count}...")
#                 # Wait for reviews to load
#                 time.sleep(5)  # Give more time for page to load
                
#                 tables = driver.page_source
                
#                 soup = BeautifulSoup(tables, 'html.parser')
#                 ret_bodies = soup.find_all('span', {'data-hook': 'review-body'})
#                 ret_titles = soup.find_all('a', {'data-hook': 'review-title'})  
#                 ret_ratings = soup.find_all('span', class_='a-icon-alt')
                
#                 print(f"Found {len(ret_bodies)} reviews on this page")
                
#                 # Process found reviews up to target_reviews
#                 for i in range(min(len(ret_bodies), target_reviews - len(reviews))):
#                     review_titles.append(ret_titles[i].text.strip())
#                     reviews.append(ret_bodies[i].text.strip())
#                     ratings.append(ret_ratings[i].text.strip())
                
#                 # Stop if we've reached our target
#                 if len(reviews) >= target_reviews:
#                     print(f"‚úÖ Reached target of {target_reviews} reviews.")
#                     break
                    
#                 # Try to go to next page
#                 try:
#                     print("Looking for next page button...")
#                     next_button = WebDriverWait(driver, 10).until(
#                         EC.element_to_be_clickable((By.CSS_SELECTOR, "li.a-last a"))
#                     )
#                     next_button.click()
#                     page_count += 1
#                     time.sleep(2)  # Brief pause after clicking
#                 except Exception as e:
#                     print(f"No more pages available: {str(e)}")
#                     break
                    
#             print(f"Scraping complete. Retrieved {len(reviews)} reviews.")
            
#         except Exception as e:
#             print(f"Error during scraping: {str(e)}")
#         finally:
#             driver.quit()
        
#         # Limit to target_reviews in case we got more
#         return review_titles[:target_reviews], reviews[:target_reviews], ratings[:target_reviews]

#     def _setup_driver(self):
#         """Set up the Selenium WebDriver with appropriate options"""
#         chrome_options = Options()
#         chrome_options.add_argument('--disable-gpu')
#         chrome_options.add_argument('--disable-dev-shm-usage')
#         chrome_options.add_argument('--no-sandbox')
#         chrome_options.add_argument('--log-level=3')  # Minimize logging
#         chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
#         chrome_options.add_experimental_option('useAutomationExtension', False)
#         chrome_options.add_experimental_option("detach", True)  # Keep browser open after script finishes
#         prefs = {
#             "credentials_enable_service": False,         # Disables the password manager
#             "profile.password_manager_enabled": False    # Disables the password manager UI
#         }
#         chrome_options.add_experimental_option("prefs", prefs)
        
#         return webdriver.Chrome(options=chrome_options)

#     def _save_to_csv(self, titles, reviews, ratings, product_name="Amazon Product"):
#         """Save reviews to CSV in format compatible with preprocessing.ipynb"""
#         # Create a DataFrame
#         df = pd.DataFrame({
#             "id": range(1, len(titles) + 1),
#             "name": product_name,
#             "brand": "Amazon",
#             "categories": "Product",
#             "primaryCategories": "Product",
#             "reviews.doRecommend": [True if r.startswith("5") or r.startswith("4") else False for r in ratings],
#             "reviews.rating": [float(r.split()[0]) if r and r[0].isdigit() else 0 for r in ratings],
#             "reviews.text": reviews,
#             "reviews.title": titles
#         })
        
#         # Save to CSV
#         filename = "scraped_reviews.csv"
#         df.to_csv(filename, index=False)
#         print(f"‚úÖ Saved {len(df)} reviews to {filename}")
#         return filename

# Version 2: 

# from typing import ClassVar, Optional, Dict, Any, Type
# from pydantic import BaseModel, Field, PrivateAttr
# from crewai.tools import BaseTool
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import time
# import os
# import pandas as pd
# from dotenv import load_dotenv

# # Define the input schema using Pydantic
# class AmazonScraperSchema(BaseModel):
#     product_url: str = Field(
#         description="Amazon product URL to scrape reviews from"
#     )
#     target_reviews: int = Field(
#         description="Number of reviews to scrape"
#     )
#     product_name: Optional[str] = Field(
#         default=None,
#         description="Optional name to use for the product (default: extracted from page)"
#     )

# class AmazonScraperTool(BaseTool):
#     """Tool that scrapes customer reviews from Amazon product pages"""

#     name: str = "amazon_scraper_tool"
#     description: str = "Scrapes customer reviews from Amazon product pages"
#     args_schema: Type[BaseModel] = AmazonScraperSchema

#     # Use class variables to store credentials
#     _username: str = PrivateAttr()
#     _password: str = PrivateAttr()

#     def __init__(self):
#         # First initialize the parent class
#         super().__init__()

#         # Load environment variables
#         load_dotenv()
#         self._username = os.getenv('NUMBER')
#         self._password = os.getenv('PASSWORD')

#         # Check if credentials are available
#         if not self._username or not self._password:
#             print("‚ö†Ô∏è Error: Missing credentials in .env file")
#             print("Please ensure your .env file contains:")
#             print("NUMBER=your_email_or_phone_number")
#             print("PASSWORD=your_amazon_password")
#         else:
#             print(f"‚úì Credentials loaded successfully (username: {self._username[:2]}...{self._username[-2:]})")
    
#     def _run(self, product_url: str, target_reviews: int = 50, product_name: Optional[str] = None) -> str:
        
#         """Run the tool with the given inputs."""
#         print(f"DEBUG: Received URL to scrape: {product_url}")
#         # Check for credentials
#         if not self._username or not self._password:
#             return "Error: Missing Amazon credentials in .env file"
#         # Check for credentials
#         # if not AmazonScraperTool._username or not AmazonScraperTool._password:
#         #     return "Error: Missing Amazon credentials in .env file"
        
#         print(f"Starting to scrape {target_reviews} reviews from: {product_url}")
        
#         # Set up the driver and scrape reviews
#         try:
#             reviews_titles, reviews_texts, ratings = self._perform_scraping(product_url, target_reviews)
            
#             # Save to CSV
#             if not product_name:
#                 product_name = "Amazon Product"  # Default name if not provided
                
#             csv_path = self._save_to_csv(reviews_titles, reviews_texts, ratings, product_name)
            
#             return f"Successfully scraped {len(reviews_texts)} reviews. Data saved to {csv_path}"
        
#         except Exception as e:
#             return f"Error during scraping: {str(e)}"

#     def _perform_scraping(self, product_url : str, target_reviews):
#         """Internal method to handle the actual scraping logic"""
#         driver = self._setup_driver()
#         review_titles = []
#         reviews = []
#         ratings = []

#         try:
#             print(f"Opening URL: {product_url}")
#             driver.get(product_url)

#             # Wait and click on the "See all reviews" link
#             print("Looking for 'See all reviews' link...")
#             WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.XPATH, '//*[@id="reviews-medley-footer"]/div[2]/a'))
#             ).click()
            
#             # Wait for email input field and enter email
#             print("Logging in with provided credentials...")
#             email_input = WebDriverWait(driver, 20).until(
#                 EC.presence_of_element_located((By.ID, "ap_email_login"))
#             )
#             print(f"Using username: {self._username[:2]}...{self._username[-2:]}")
#             email_input.send_keys(self._username)
            
#             enter_button = WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.ID, "continue"))
#             )
#             enter_button.click()
            
#             password_input = WebDriverWait(driver, 20).until(
#                 EC.presence_of_element_located((By.ID, "ap_password"))
#             )
#             password_input.send_keys(self._password)
            
#             sign_in_button = WebDriverWait(driver, 20).until(
#                 EC.element_to_be_clickable((By.ID, "signInSubmit"))
#             )
#             sign_in_button.click()
            
#             page_count = 1
#             max_pages = 10  # Maximum number of pages to scrape
            
#             # Continue until we reach target_reviews or max_pages
#             while len(reviews) < target_reviews and page_count <= max_pages:
#                 print(f"Scraping page {page_count}...")
#                 # Wait for reviews to load
#                 time.sleep(5)  # Give more time for page to load
                
#                 tables = driver.page_source
                
#                 soup = BeautifulSoup(tables, 'html.parser')
#                 ret_bodies = soup.find_all('span', {'data-hook': 'review-body'})
#                 ret_titles = soup.find_all('a', {'data-hook': 'review-title'})  
#                 ret_ratings = soup.find_all('span', class_='a-icon-alt')
                
#                 print(f"Found {len(ret_bodies)} reviews on this page")
                
#                 # Process found reviews up to target_reviews
#                 for i in range(min(len(ret_bodies), target_reviews - len(reviews))):
#                     review_titles.append(ret_titles[i].text.strip())
#                     reviews.append(ret_bodies[i].text.strip())
#                     ratings.append(ret_ratings[i].text.strip())
                
#                 # Stop if we've reached our target
#                 if len(reviews) >= target_reviews:
#                     print(f"‚úÖ Reached target of {target_reviews} reviews.")
#                     break
                    
#                 # Try to go to next page
#                 try:
#                     print("Looking for next page button...")
#                     next_button = WebDriverWait(driver, 10).until(
#                         EC.element_to_be_clickable((By.CSS_SELECTOR, "li.a-last a"))
#                     )
#                     next_button.click()
#                     page_count += 1
#                     time.sleep(2)  # Brief pause after clicking
#                 except Exception as e:
#                     print(f"No more pages available: {str(e)}")
#                     break
                    
#             print(f"Scraping complete. Retrieved {len(reviews)} reviews.")
            
#         except Exception as e:
#             print(f"Error during scraping: {str(e)}")
#         finally:
#             driver.quit()
        
#         # Limit to target_reviews in case we got more
#         return review_titles[:target_reviews], reviews[:target_reviews], ratings[:target_reviews]


#     def _setup_driver(self):
#         """Set up the Selenium WebDriver with appropriate options"""
#         chrome_options = Options()
#         chrome_options.add_argument('--disable-gpu')
#         chrome_options.add_argument('--disable-dev-shm-usage')
#         chrome_options.add_argument('--no-sandbox')
#         chrome_options.add_argument('--log-level=3')  # Minimize logging
#         chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
#         chrome_options.add_experimental_option('useAutomationExtension', False)
#         prefs = {
#             "credentials_enable_service": False,         # Disables the password manager
#             "profile.password_manager_enabled": False    # Disables the password manager UI
#         }
#         chrome_options.add_experimental_option("prefs", prefs)
        
#         return webdriver.Chrome(options=chrome_options)

#     def _save_to_csv(self, titles, reviews, ratings, product_name="Amazon Product"):
#         """Save reviews to CSV in format compatible with preprocessing.ipynb"""
#         # Create a DataFrame
#         df = pd.DataFrame({
#             "id": range(1, len(titles) + 1),
#             "name": product_name,
#             "brand": "Amazon",
#             "categories": "Product",
#             "primaryCategories": "Product",
#             "reviews.doRecommend": [True if r.startswith("5") or r.startswith("4") else False for r in ratings],
#             "reviews.rating": [float(r.split()[0]) if r and r[0].isdigit() else 0 for r in ratings],
#             "reviews.text": reviews,
#             "reviews.title": titles
#         })
        
#         # Save as reviews_cleaned.csv for compatibility with existing code
#         compatibility_filename = "scraped_reviews.csv"
#         df.to_csv(compatibility_filename, index=False)
#         print(f"‚úÖ Saved {len(df)} reviews to {compatibility_filename} for analysis")
        
#         return compatibility_filename

# Version 3:

from typing import ClassVar, Optional, Dict, Any, Type
from pydantic import BaseModel, Field, PrivateAttr
from crewai.tools import BaseTool
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
import os
import pandas as pd
import re
from dotenv import load_dotenv
import logging

# Set up logging for better debugging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    filename='amazon_scraper.log')
logger = logging.getLogger('amazon_scraper')

# Define the input schema using Pydantic
class AmazonScraperSchema(BaseModel):
    url: str = Field(
        description="Amazon product URL to scrape reviews from"
    )
    target_reviews: int = Field(
        description="Number of reviews to scrape"
    )
    product_name: Optional[str] = Field(
        default=None,
        description="Optional name to use for the product (default: extracted from page)"
    )

class AmazonScraperTool(BaseTool):
    """Tool that scrapes customer reviews from Amazon product pages"""

    name: str = "amazon_scraper_tool"
    description: str = "Scrapes customer reviews from Amazon product pages"
    args_schema: Type[BaseModel] = AmazonScraperSchema

    # Use class variables to store credentials
    _username: str = PrivateAttr()
    _password: str = PrivateAttr()

    def __init__(self):
        # First initialize the parent class
        super().__init__()

        # Load environment variables
        load_dotenv()
        self._username = os.getenv('NUMBER')
        self._password = os.getenv('PASSWORD')

        # Check if credentials are available
        if not self._username or not self._password:
            logger.error("Missing credentials in .env file")
            print("‚ö†Ô∏è Error: Missing credentials in .env file")
            print("Please ensure your .env file contains:")
            print("NUMBER=your_email_or_phone_number")
            print("PASSWORD=your_amazon_password")
        else:
            logger.info(f"Credentials loaded successfully (username: {self._username[:2]}...{self._username[-2:]})")
            print(f"‚úì Credentials loaded successfully (username: {self._username[:2]}...{self._username[-2:]})")
    
    def _run(self, url: str, target_reviews: int, product_name: Optional[str] = None) -> str:
        """Run the tool with the given inputs."""
        print(f"DEBUG: Received URL to scrape: {url}")
    
        # Ensure we're keeping the original URL domain intact
        if "amazon.in" in url:
            print("Using Indian Amazon domain (amazon.in)")
        
        # Check for credentials
        if not self._username or not self._password:
            return "Error: Missing Amazon credentials in .env file"
        
        # Preserve the original URL - ensure we're not modifying it
        original_url = url
        logger.info(f"Starting to scrape {target_reviews} reviews from: {original_url}")
        print(f"Starting to scrape {target_reviews} reviews from: {original_url}")
        
        # Ensure URL is properly formatted (handle Amazon short URLs)
        if not re.match(r'^https?://', original_url):
            original_url = f"https://{original_url}"
        
        # Set up the driver and scrape reviews
        try:
            reviews_titles, reviews_texts, ratings = self._perform_scraping(url, target_reviews)
            
            # Save to CSV
            if not product_name:
                product_name = "Amazon Product"  # Default name if not provided
                
            csv_path = self._save_to_csv(reviews_titles, reviews_texts, ratings, product_name)
            
            return f"Successfully scraped {len(reviews_texts)} reviews. Data saved to {csv_path}"
        
        except Exception as e:
            error_msg = f"Error during scraping: {str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg

    def _perform_scraping(self, url: str, target_reviews):
        """Internal method to handle the actual scraping logic"""
        driver = self._setup_driver()
        review_titles = []
        reviews = []
        ratings = []

        try:
            logger.info(f"Opening URL: {url}")
            print(f"Opening URL: {url}")
            driver.get(url)
            
            # Save the initial page for debugging
            with open('initial_page.html', 'w', encoding='utf-8') as f:
                f.write(driver.page_source)
            
            # Check if we need to handle CAPTCHA
            if "captcha" in driver.page_source.lower():
                logger.info("CAPTCHA detected! Please solve the CAPTCHA manually...")
                print("üîç CAPTCHA detected! Please solve the CAPTCHA manually...")
                # Wait for manual CAPTCHA solution
                time.sleep(10)  # Give user time to notice
                input("Press Enter after solving CAPTCHA...")
            
            # Wait and click on the "See all reviews" link
            try:
                logger.info("Looking for 'See all reviews' link...")
                print("Looking for 'See all reviews' link...")
                
                # First try the standard path
                try:
                    all_reviews_link = WebDriverWait(driver, 15).until(
                        EC.element_to_be_clickable((By.XPATH, '//*[@id="reviews-medley-footer"]/div[2]/a'))
                    )
                    all_reviews_link.click()
                    logger.info("Clicked on 'See all reviews' link")
                except Exception as e:
                    logger.warning(f"Standard 'See all reviews' link not found: {str(e)}")
                    
                    # Try alternative selectors
                    try:
                        # Try to find by text content
                        all_reviews_link = WebDriverWait(driver, 10).until(
                            EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), "See all reviews") or contains(text(), "customer reviews")]'))
                        )
                        all_reviews_link.click()
                        logger.info("Clicked on alternative 'See all reviews' link")
                    except Exception as e2:
                        logger.warning(f"Alternative 'See all reviews' link not found: {str(e2)}")
                        
                        # Last resort - try to directly navigate to reviews page
                        if "/dp/" in url:
                            product_id = re.search(r'/dp/([A-Z0-9]+)', url).group(1)
                            domain = re.search(r'https?://(?:www\.)?([^/]+)', url).group(0)
                            reviews_url = f"{domain}/product-reviews/{product_id}"
                            logger.info(f"Attempting to navigate directly to: {reviews_url}")
                            driver.get(reviews_url)
            except Exception as e:
                logger.error(f"Failed to navigate to reviews: {str(e)}")
                # Continue anyway, we might already be on a reviews page
            
            # Check if login is required
            if "ap_email_login" in driver.page_source or "ap_email" in driver.page_source:
                logger.info("Login form detected, attempting to log in...")
                print("Login form detected, attempting to log in...")
                
                # Try the regular email field first
                try:
                    email_input = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.ID, "ap_email_login"))
                    )
                except:
                    try:
                        email_input = WebDriverWait(driver, 10).until(
                            EC.presence_of_element_located((By.ID, "ap_email"))
                        )
                    except Exception as e:
                        logger.error(f"Could not find email input: {str(e)}")
                        print("Could not find email input field")
                        raise
                
                logger.info(f"Using username: {self._username[:2]}...{self._username[-2:]}")
                email_input.send_keys(self._username)
                
                # Try different continue button IDs
                try:
                    enter_button = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable((By.ID, "continue"))
                    )
                except:
                    try:
                        enter_button = WebDriverWait(driver, 10).until(
                            EC.element_to_be_clickable((By.ID, "sign-in-button"))
                        )
                    except Exception as e:
                        logger.error(f"Could not find continue button: {str(e)}")
                        print("Could not find continue button")
                        raise
                        
                enter_button.click()
                
                # Wait for password field
                try:
                    password_input = WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.ID, "ap_password"))
                    )
                    password_input.send_keys(self._password)
                    
                    sign_in_button = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable((By.ID, "signInSubmit"))
                    )
                    sign_in_button.click()
                    logger.info("Login credentials submitted")
                except Exception as e:
                    logger.error(f"Error during login: {str(e)}")
                    print(f"Error during login: {str(e)}")
                    # Continue anyway, we might still be able to access reviews
            
            page_count = 1
            max_pages = 10  # Maximum number of pages to scrape
            
            # Continue until we reach target_reviews or max_pages
            while len(reviews) < target_reviews and page_count <= max_pages:
                logger.info(f"Scraping page {page_count}...")
                print(f"Scraping page {page_count}...")
                
                # Wait for reviews to load
                time.sleep(5)  # Give more time for page to load
                
                # Save the page source for debugging
                with open(f'reviews_page_{page_count}.html', 'w', encoding='utf-8') as f:
                    f.write(driver.page_source)
                
                tables = driver.page_source
                
                soup = BeautifulSoup(tables, 'html.parser')
                ret_bodies = soup.find_all('span', {'data-hook': 'review-body'})
                ret_titles = soup.find_all('a', {'data-hook': 'review-title'})
                
                # Try alternative selectors if nothing found
                if not ret_bodies:
                    ret_bodies = soup.find_all('div', {'class': 'review-text'})
                if not ret_titles:
                    ret_titles = soup.find_all('a', {'class': 'review-title'})
                
                # Rating might have different selectors
                ret_ratings = soup.find_all('span', class_='a-icon-alt')
                if not ret_ratings:
                    ret_ratings = soup.find_all('i', {'data-hook': 'review-star-rating'})
                
                logger.info(f"Found {len(ret_bodies)} reviews on this page")
                print(f"Found {len(ret_bodies)} reviews on this page")
                
                # Process found reviews up to target_reviews
                for i in range(min(len(ret_bodies), min(len(ret_titles), target_reviews - len(reviews)))):
                    try:
                        review_titles.append(ret_titles[i].text.strip())
                        reviews.append(ret_bodies[i].text.strip())
                        
                        # Handle ratings carefully
                        if i < len(ret_ratings):
                            rating_text = ret_ratings[i].text.strip()
                            ratings.append(rating_text)
                        else:
                            # Default rating if not found
                            ratings.append("Not specified")
                    except Exception as e:
                        logger.error(f"Error parsing review {i}: {str(e)}")
                
                # Stop if we've reached our target
                if len(reviews) >= target_reviews:
                    logger.info(f"‚úÖ Reached target of {target_reviews} reviews.")
                    print(f"‚úÖ Reached target of {target_reviews} reviews.")
                    break
                    
                # Try to go to next page
                try:
                    logger.info("Looking for next page button...")
                    print("Looking for next page button...")
                    
                    # Try multiple selectors for the next button
                    try:
                        next_button = WebDriverWait(driver, 10).until(
                            EC.element_to_be_clickable((By.CSS_SELECTOR, "li.a-last a"))
                        )
                    except:
                        try:
                            next_button = WebDriverWait(driver, 10).until(
                                EC.element_to_be_clickable((By.XPATH, '//a[contains(text(), "Next page")]'))
                            )
                        except:
                            try:
                                # Try another common Amazon pagination pattern
                                next_button = WebDriverWait(driver, 10).until(
                                    EC.element_to_be_clickable((By.XPATH, '//span[contains(@class, "s-pagination-next")]'))
                                )
                            except Exception as e:
                                logger.warning(f"No next page button found: {str(e)}")
                                print("No more pages available")
                                break
                    
                    next_button.click()
                    page_count += 1
                    time.sleep(3)  # Longer pause after clicking for page load
                except Exception as e:
                    logger.warning(f"Failed to navigate to next page: {str(e)}")
                    print(f"No more pages available: {str(e)}")
                    break
                    
            logger.info(f"Scraping complete. Retrieved {len(reviews)} reviews.")
            print(f"Scraping complete. Retrieved {len(reviews)} reviews.")
            
        except Exception as e:
            logger.error(f"Error during scraping: {str(e)}", exc_info=True)
            print(f"Error during scraping: {str(e)}")
            raise
        finally:
            driver.quit()
        
        # Limit to target_reviews in case we got more
        return review_titles[:target_reviews], reviews[:target_reviews], ratings[:target_reviews]

    def _setup_driver(self):
        """Set up the Selenium WebDriver with appropriate options"""
        chrome_options = Options()
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--log-level=3')  # Minimize logging
        
        # Add user agent to avoid bot detection
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36')
        
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        prefs = {
            "credentials_enable_service": False,         # Disables the password manager
            "profile.password_manager_enabled": False    # Disables the password manager UI
        }
        chrome_options.add_experimental_option("prefs", prefs)
        
        return webdriver.Chrome(options=chrome_options)

    def _save_to_csv(self, titles, reviews, ratings, product_name="Amazon Product"):
        """Save reviews to CSV in format compatible with preprocessing.ipynb"""
        # Create a DataFrame
        df = pd.DataFrame({
            "id": range(1, len(titles) + 1),
            "name": product_name,
            "brand": "Amazon",
            "categories": "Product",
            "primaryCategories": "Product",
            "reviews.doRecommend": [True if r.startswith("5") or r.startswith("4") else False for r in ratings],
            "reviews.rating": [float(r.split()[0]) if r and r[0].isdigit() else 0 for r in ratings],
            "reviews.text": reviews,
            "reviews.title": titles
        })
        
        # Save as scraped_reviews.csv
        compatibility_filename = "scraped_reviews.csv"
        df.to_csv(compatibility_filename, index=False)
        logger.info(f"‚úÖ Saved {len(df)} reviews to {compatibility_filename} for analysis")
        print(f"‚úÖ Saved {len(df)} reviews to {compatibility_filename} for analysis")
        
        return compatibility_filename




# from typing import Optional
# from langchain.tools import tool
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from bs4 import BeautifulSoup
# import time
# import os
# import pandas as pd
# from dotenv import load_dotenv

# # Load environment variables once at module level
# load_dotenv()
# _USERNAME = os.getenv('NUMBER')
# _PASSWORD = os.getenv('PASSWORD')

# @tool
# def amazon_scraper_tool(url: str, target_reviews: int = 50, product_name: Optional[str] = None) -> str:
#     """Scrapes customer reviews from Amazon product pages.
    
#     Args:
#         url: Amazon product URL to scrape reviews from
#         target_reviews: Number of reviews to scrape (default: 50, max: 500)
#         product_name: Optional name to use for the product (default: extracted from page)
        
#     Returns:
#         Path to the CSV file containing the scraped reviews
#     """
#     # Check for credentials
#     if not _USERNAME or not _PASSWORD:
#         return "Error: Missing Amazon credentials in .env file"
    
#     print(f"Starting to scrape {target_reviews} reviews from: {url}")
    
#     # Set up the driver and scrape reviews
#     try:
#         reviews_titles, reviews_texts, ratings = _perform_scraping(url, target_reviews)
        
#         # Save to CSV
#         if not product_name:
#             product_name = "Amazon Product"  # Default name if not provided
            
#         csv_path = _save_to_csv(reviews_titles, reviews_texts, ratings, product_name)
        
#         return f"Successfully scraped {len(reviews_texts)} reviews. Data saved to {csv_path}"
    
#     except Exception as e:
#         return f"Error during scraping: {str(e)}"

# def _perform_scraping(url, target_reviews):
#     """Internal method to handle the actual scraping logic"""
#     driver = _setup_driver()
#     review_titles = []
#     reviews = []
#     ratings = []

#     try:
#         print(f"Opening URL: {url}")
#         driver.get(url)

#         # Wait and click on the "See all reviews" link
#         print("Looking for 'See all reviews' link...")
#         WebDriverWait(driver, 20).until(
#             EC.element_to_be_clickable((By.XPATH, '//*[@id="reviews-medley-footer"]/div[2]/a'))
#         ).click()
        
#         # Wait for email input field and enter email
#         print("Logging in with provided credentials...")
#         email_input = WebDriverWait(driver, 20).until(
#             EC.presence_of_element_located((By.ID, "ap_email_login"))
#         )
#         print(f"Using username: {_USERNAME[:2]}...{_USERNAME[-2:]}")
#         email_input.send_keys(_USERNAME)
        
#         enter_button = WebDriverWait(driver, 20).until(
#             EC.element_to_be_clickable((By.ID, "continue"))
#         )
#         enter_button.click()
        
#         password_input = WebDriverWait(driver, 20).until(
#             EC.presence_of_element_located((By.ID, "ap_password"))
#         )
#         password_input.send_keys(_PASSWORD)
        
#         sign_in_button = WebDriverWait(driver, 20).until(
#             EC.element_to_be_clickable((By.ID, "signInSubmit"))
#         )
#         sign_in_button.click()
        
#         page_count = 1
#         max_pages = 10  # Maximum number of pages to scrape
        
#         # Continue until we reach target_reviews or max_pages
#         while len(reviews) < target_reviews and page_count <= max_pages:
#             print(f"Scraping page {page_count}...")
#             # Wait for reviews to load
#             time.sleep(5)  # Give more time for page to load
            
#             tables = driver.page_source
            
#             soup = BeautifulSoup(tables, 'html.parser')
#             ret_bodies = soup.find_all('span', {'data-hook': 'review-body'})
#             ret_titles = soup.find_all('a', {'data-hook': 'review-title'})  
#             ret_ratings = soup.find_all('span', class_='a-icon-alt')
            
#             print(f"Found {len(ret_bodies)} reviews on this page")
            
#             # Process found reviews up to target_reviews
#             for i in range(min(len(ret_bodies), target_reviews - len(reviews))):
#                 review_titles.append(ret_titles[i].text.strip())
#                 reviews.append(ret_bodies[i].text.strip())
#                 ratings.append(ret_ratings[i].text.strip())
            
#             # Stop if we've reached our target
#             if len(reviews) >= target_reviews:
#                 print(f"‚úÖ Reached target of {target_reviews} reviews.")
#                 break
                
#             # Try to go to next page
#             try:
#                 print("Looking for next page button...")
#                 next_button = WebDriverWait(driver, 10).until(
#                     EC.element_to_be_clickable((By.CSS_SELECTOR, "li.a-last a"))
#                 )
#                 next_button.click()
#                 page_count += 1
#                 time.sleep(2)  # Brief pause after clicking
#             except Exception as e:
#                 print(f"No more pages available: {str(e)}")
#                 break
                
#         print(f"Scraping complete. Retrieved {len(reviews)} reviews.")
        
#     except Exception as e:
#         print(f"Error during scraping: {str(e)}")
#     finally:
#         driver.quit()
    
#     # Limit to target_reviews in case we got more
#     return review_titles[:target_reviews], reviews[:target_reviews], ratings[:target_reviews]

# def _setup_driver():
#     """Set up the Selenium WebDriver with appropriate options"""
#     chrome_options = Options()
#     chrome_options.add_argument('--disable-gpu')
#     chrome_options.add_argument('--disable-dev-shm-usage')
#     chrome_options.add_argument('--no-sandbox')
#     chrome_options.add_argument('--log-level=3')  # Minimize logging
#     chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
#     chrome_options.add_experimental_option('useAutomationExtension', False)
#     prefs = {
#         "credentials_enable_service": False,         # Disables the password manager
#         "profile.password_manager_enabled": False    # Disables the password manager UI
#     }
#     chrome_options.add_experimental_option("prefs", prefs)
    
#     return webdriver.Chrome(options=chrome_options)

# def _save_to_csv(titles, reviews, ratings, product_name="Amazon Product"):
#     """Save reviews to CSV in format compatible with preprocessing.ipynb"""
#     # Create a DataFrame
#     df = pd.DataFrame({
#         "id": range(1, len(titles) + 1),
#         "name": product_name,
#         "brand": "Amazon",
#         "categories": "Product",
#         "primaryCategories": "Product",
#         "reviews.doRecommend": [True if r.startswith("5") or r.startswith("4") else False for r in ratings],
#         "reviews.rating": [float(r.split()[0]) if r and r[0].isdigit() else 0 for r in ratings],
#         "reviews.text": reviews,
#         "reviews.title": titles
#     })
    
#     # Save as reviews_cleaned.csv for compatibility with existing code
#     compatibility_filename = "reviews_cleaned.csv"
#     df.to_csv(compatibility_filename, index=False)
#     print(f"‚úÖ Saved {len(df)} reviews to {compatibility_filename} for analysis")
    
#     return compatibility_filename